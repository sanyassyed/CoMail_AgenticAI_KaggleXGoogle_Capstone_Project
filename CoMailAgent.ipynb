{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Capstone Project - **Concierge Agent** Category \n\n## *CoMailAgent* â€“ Automated Resume Tailoring & Cold Email Job Outreach Agent \n- by Supriya & Sanya\n\nFind below the code & instructions for this project","metadata":{}},{"cell_type":"markdown","source":"### Step 1: Configuring Google API Keys\nThis notebook uses the [Gemini API](https://ai.google.dev/gemini-api/), which requires an API key.","metadata":{}},{"cell_type":"markdown","source":"### Step 2: Import ADK components\n\nNow, importing the specific components we will need from the Agent Development Kit and the Generative AI library. This keeps the code organized and ensures we have access to the necessary building blocks.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nFILE_ID = user_secrets.get_secret(\"FILE_ID\")\nSPREADSHEET_ID = user_secrets.get_secret(\"SPREADSHEET_ID\")\ntry:\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"âœ… Setup and authentication complete.\")\nexcept Exception as e:\n    print(\n        f\"ðŸ”‘ Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' to your Kaggle secrets. Details: {e}\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:23:10.880898Z","iopub.execute_input":"2025-12-01T04:23:10.882119Z","iopub.status.idle":"2025-12-01T04:23:11.585394Z","shell.execute_reply.started":"2025-12-01T04:23:10.882084Z","shell.execute_reply":"2025-12-01T04:23:11.584019Z"}},"outputs":[{"name":"stdout","text":"âœ… Setup and authentication complete.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install fpdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:30:36.940544Z","iopub.execute_input":"2025-12-01T04:30:36.941559Z","iopub.status.idle":"2025-12-01T04:30:44.227292Z","shell.execute_reply.started":"2025-12-01T04:30:36.941521Z","shell.execute_reply":"2025-12-01T04:30:44.225836Z"}},"outputs":[{"name":"stdout","text":"Collecting fpdf\n  Downloading fpdf-1.7.2.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: fpdf\n  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=1b0e4a2b4b7aa7094b766a5b5bf2edde6f6ef370d5da892d2531a114eb08e05d\n  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\nSuccessfully built fpdf\nInstalling collected packages: fpdf\nSuccessfully installed fpdf-1.7.2\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import pandas as pd\nimport requests \nimport os\nfrom fpdf import FPDF\nimport io","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:40:36.100230Z","iopub.execute_input":"2025-12-01T04:40:36.100572Z","iopub.status.idle":"2025-12-01T04:40:36.105848Z","shell.execute_reply.started":"2025-12-01T04:40:36.100549Z","shell.execute_reply":"2025-12-01T04:40:36.104952Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from google.genai import types\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.tools import google_search, AgentTool, ToolContext\nfrom google.adk.code_executors import BuiltInCodeExecutor\n\nprint(\"âœ… ADK components imported successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:23:21.877515Z","iopub.execute_input":"2025-12-01T04:23:21.877866Z","iopub.status.idle":"2025-12-01T04:24:16.002104Z","shell.execute_reply.started":"2025-12-01T04:23:21.877837Z","shell.execute_reply":"2025-12-01T04:24:16.000976Z"}},"outputs":[{"name":"stdout","text":"âœ… ADK components imported successfully.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Step 3: Configure Retry Options\n\nWhen working with LLMs, you may encounter transient errors like rate limits or temporary service unavailability. Retry options automatically handle these failures by retrying the request with exponential backoff.","metadata":{}},{"cell_type":"code","source":"retry_config = types.HttpRetryOptions(\n    attempts=5,  # Maximum retry attempts\n    exp_base=7,  # Delay multiplier\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:24:34.817532Z","iopub.execute_input":"2025-12-01T04:24:34.819224Z","iopub.status.idle":"2025-12-01T04:24:34.825403Z","shell.execute_reply.started":"2025-12-01T04:24:34.819185Z","shell.execute_reply":"2025-12-01T04:24:34.824100Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"1. root_agent - Uses the \n1. get_task_items() tool\n    * Goes through the db and returns a dictionary with \n2. cover_letter_agent - for writing a custom cover letter\n    * **Extracts keywords and role expectations** from the job description.\n    * ~~**Tailors the resume** to highlight relevant skills and achievements.~~\n    * **Generates a professional, customized cover letter** for each job.\n3. email_agent - for drafting a recruiter outreach email\n    * **Creates a personalized cold outreach email** addressed to the recruiter or hiring manager.\n    * **Packages attachments** (tailored resume + cover letter).\n    * **Sends the email automatically**, or optionally requests user approval before sending.","metadata":{}},{"cell_type":"code","source":"# print(os.path.join(os.getcwd(), 'filename.pdf'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:52:50.394488Z","iopub.execute_input":"2025-12-01T03:52:50.394785Z","iopub.status.idle":"2025-12-01T03:52:50.401339Z","shell.execute_reply.started":"2025-12-01T03:52:50.394764Z","shell.execute_reply":"2025-12-01T03:52:50.399764Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/filename.pdf\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# # testing\n# google_sheet_url = f\"https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/export?format=xlsx\"\n# job_dataset = pd.read_excel(google_sheet_url, sheet_name=\"Sheet1\").to_dict(orient='list')\n# print(job_dataset)\n# print(job_dataset[0:1].to_dict(orient='list'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T23:17:17.559650Z","iopub.execute_input":"2025-11-30T23:17:17.560584Z","iopub.status.idle":"2025-11-30T23:17:17.567204Z","shell.execute_reply.started":"2025-11-30T23:17:17.560549Z","shell.execute_reply":"2025-11-30T23:17:17.565944Z"}},"outputs":[{"name":"stdout","text":"{'job_id': [1], 'job_description': ['Description 1'], 'recruiter_name': ['rachael x'], 'recruiter_email': ['rachael@reatil.com']}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# # testing\n# format = 'txt'\n# url = f\"https://docs.google.com/document/d/{FILE_ID}/export?format=txt\"\n# # The export URL format for Google Docs\n# # url = f\"docs.google.com{format}&id={FILE_ID}\"\n    \n# try:\n#     response = requests.get(url)\n#     response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n#     print(response.text)\n# except requests.exceptions.RequestException as e:\n#     print(f\"Error during download: {e}\")\n#     #return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T23:42:19.934734Z","iopub.execute_input":"2025-11-30T23:42:19.935081Z","iopub.status.idle":"2025-11-30T23:42:22.280001Z","shell.execute_reply.started":"2025-11-30T23:42:19.935057Z","shell.execute_reply":"2025-11-30T23:42:22.278988Z"}},"outputs":[{"name":"stdout","text":"ï»¿Dear Recruitment Team,\nI am writing to express my interest in data-focused opportunitiesâ€”whether as a Data Engineer or Data Analystâ€”at your organization. As a data enthusiast with a solid foundation in data science and analytics, Iâ€™ve recently deepened my expertise by completing rigorous data engineering programs and applying those skills in hands-on capstone and pro bono projects designed to solve real-world challenges.\nMy earlier experience as a Data Scientist and Data Engineer at DeepMiner gave me a strong grounding in building ETL pipelines, developing NLP models, and creating analytics dashboards. Iâ€™ve since expanded my capabilities through continuous learning and project-based work aligned with modern data engineering and analytics practices.\nRecent highlights include:\n* Developing end-to-end ELT pipelines using Python, SQL, Docker, and AWS/GCP, reducing processing latency and simulating production-grade workflows.\n* Designing secure cloud-native data pipelines with Google Cloud Functions and Secret Manager, improving data consistency and security.\n* Building interactive dashboards in Power BI, enhancing stakeholder insights and reducing reporting time by over 60%.\n* Leading pro bono data projects for operational analytics in the logistics domain, streamlining data workflows and improving decision-making accuracy.\n* Applying data modeling, version control, governance principles, and AI/NLP techniques across projects to ensure transparency, reliability, and scalable insights.\nThrough this journey, Iâ€™ve kept my skills relevantâ€”working with both structured and semi-structured data, and applying cloud-native, open-source, and AI/ML technologies that align with modern data stacks. I bring a proactive, collaborative mindset and a passion for solving data problems that deliver tangible business value.\nI would welcome the opportunity to contribute to your team and discuss how my skills and recent project work align with your data initiatives.\nThank you for your time and consideration. I can be reached at candidate@thieremail.com. I look forward to connecting with you.\nSincerely,\nAm Leeman\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Tool that gets the job data\ndef get_job_data_method() -> dict[list]:\n    \"\"\"Downloads the data from google sheets which contains information about the jobs and recruiter.\n\n        Returns:\n            Dictionary with status and job_data dictionary.\n            Success: {\"status\": \"success\", \"job_data\": {'job_id': [1, 2, 3], \n                'job_description':[\"Description 1\", \"Description 2\", \"Description 3\"], \n                'recruiter_name': ['racheal x', 'ross y', 'joey z'], \n                'recruiter_email': ['rachael@retail.com', 'ross@museum.com', 'joey@restaurant.com']}}\n            Error: {\"status\": \"error\", \"error_message\": \"Could not get job data from google sheets\"}\n        \"\"\"\n    google_sheet_url = f\"https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/export?format=xlsx\"\n    try:\n        # get data from google sheets\n        job_data = pd.read_excel(google_sheet_url, sheet_name=\"Sheet1\")[0:1].to_dict(orient='list')\n        if job_data:\n            return {'status': 'success', 'job_data':job_data}\n        else:\n            return {'status': 'error', 'job_data':f\"Could not retrive job data as no data available in the db\"}\n    except Exception as e:\n        return {'status': 'error', 'job_data':f\"Could not retrive job data due to the following error: {e}\"}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:34:23.545830Z","iopub.execute_input":"2025-12-01T04:34:23.546552Z","iopub.status.idle":"2025-12-01T04:34:23.554206Z","shell.execute_reply.started":"2025-12-01T04:34:23.546524Z","shell.execute_reply":"2025-12-01T04:34:23.552744Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Tool that gets the cv\ndef get_cover_letter_method() -> str:\n    \"\"\"Downloads the cover letter from the url.\n\n        Returns:\n            String with the Cover Letter.\n            Success: {\"status\": \"success\", \"cover_letter\": \"The full Cover Letter Content\"}\n            Error: {\"status\": \"error\", \"error_message\": \"Could not get the cover letter from google docs\"}\n        \"\"\"\n    google_doc_url = f\"https://docs.google.com/document/d/{FILE_ID}/export?format=txt\"\n    try:\n        # get data from google sheets\n        response = requests.get(google_doc_url)\n        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)job_data = pd.read_excel(google_sheet_url, sheet_name=\"Sheet1\")[0:1].to_dict(orient='list')\n        if response.text:\n            return {'status': 'success', 'cover_letter':response.text}\n        else:\n            return {'status': 'error', 'job_data':f\"Could not retrive the cover letter from google docs\"}\n    except requests.exceptions.RequestException as e:\n        return {'status': 'error', 'job_data':f\"Error during download: {e}\"}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:36:31.930621Z","iopub.execute_input":"2025-12-01T04:36:31.930976Z","iopub.status.idle":"2025-12-01T04:36:31.937409Z","shell.execute_reply.started":"2025-12-01T04:36:31.930950Z","shell.execute_reply":"2025-12-01T04:36:31.936480Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# # Cover Letter Writer Agent : Writes the cover letter\n\n# cv_writer_agent = Agent(\n#     name=\"CvWriterAgent\",\n#     model=Gemini(\n#         model=\"gemini-2.5-flash-lite\",\n#         retry_options=retry_config\n#     ),\n#     # The `{blog_outline}` placeholder automatically injects the state value from the previous agent's output.\n#     instruction=\"\"\"Following this outline strictly: {blog_outline}\n#     Write a brief, 200 to 300-word blog post with an engaging and informative tone.\"\"\",\n#     tools = [get_cover_letter_method],  # The result of this agent will be stored with this key.\n# )\n\n# print(\"âœ… writer_agent created.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_pdf(text_content: str):\n    pdf = FPDF()\n    pdf.add_page()\n    pdf.set_font(\"Arial\", size=12)\n\n    # Split text into lines to handle multiline content\n    lines = text_content.split('\\n')\n    for line in lines:\n        pdf.cell(200, 10, txt=line, ln=True, align='L') # ln=True moves to the next line\n\n    # Save the PDF to a byte buffer\n    pdf_byte_buffer = io.BytesIO()\n    pdf.output(pdf_byte_buffer)\n    pdf_byte_buffer.seek(0) # Rewind the buffer to the beginning\n    return pdf_byte_buffer.getvalue()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:31:24.357809Z","iopub.execute_input":"2025-12-01T04:31:24.358205Z","iopub.status.idle":"2025-12-01T04:31:24.364929Z","shell.execute_reply.started":"2025-12-01T04:31:24.358179Z","shell.execute_reply":"2025-12-01T04:31:24.363957Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def save_text_as_pdf_method(content_to_save: str, filename: str = \"cover_letter.pdf\") -> dict[str, str]:\n    \"\"\"\n    Generates a PDF from the given text content and saves it locally.\n\n    Args:\n        content_to_save: The string content to be put into the PDF.\n        filename: The desired filename for the saved PDF artifact (e.g., \"summary.pdf\").\n                  It should end with '.pdf'.\n\n    Returns:\n        A dictionary indicating the status of the operation and the file name or an error message.\n    \"\"\"\n    print(f\"--- Tool called: save_text_as_pdf, attempting to save as '{filename}' ---\")\n\n    if not content_to_save:\n        return {\"status\": \"error\", \"message\": \"No content provided to save as PDF.\"}\n\n    try:\n        if not filename.lower().endswith(\".pdf\"):\n            filename += \".pdf\"\n            print(f\"--- Info: Appended .pdf to filename. New filename: '{filename}' ---\")\n\n        print(f\"--- Generating PDF bytes for: '{content_to_save[:100]}...' ---\")\n        pdf_bytes = generate_pdf(content_to_save)\n        print(f\"--- PDF bytes generated successfully (Size: {len(pdf_bytes)} bytes) ---\")\n\n        with open(filename, \"wb\") as f:\n            f.write(pdf_bytes)\n            print(\"Saved PDF locally.\")\n        return { \"status\": \"success\", \"message\": f\"file {filename} saved locally at {os.path.join(os.getcwd(), filename)}\" }\n    except Exception as e:\n        return { \"status\": \"error\", \"error\": str(e) }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:32:51.318634Z","iopub.execute_input":"2025-12-01T04:32:51.319083Z","iopub.status.idle":"2025-12-01T04:32:51.327049Z","shell.execute_reply.started":"2025-12-01T04:32:51.319057Z","shell.execute_reply":"2025-12-01T04:32:51.325922Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Root Agent\ncomail_agent = LlmAgent(\n    name=\"comail_agent\",\n    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n    # Updated instruction\n    instruction=\"\"\"You are a professional and highly intelligent job application assistant. You must strictly follow these steps and use the available tools to send custom cover letters for different jobs via email.\n\n  Do the following in order and follow the instructions strictly\n\n   1. Get Job Data: Use the get_job_data_method tool to get the job_data that contains information about the job id, job_description, recruiters full name which includes the first and last name and recruiters email address .\n   2. Get Cover Letter: Use the get_cover_letter_method tool to get the cover letter\n   3. Generate Cover Letter (CRITICAL): You are strictly prohibited from making up information. Using the job_data from Step 1, the cover letter from Step 2 write an updated cover letter based on job description. Do not makeup any information. Write the content of the cover letter keeping in mind it needs to be converted to PDF and therefore must have the right indentation and formatting.\n   4. Error Check: After each tool call, you must check the \"status\" field in the response. If the status is \"error\", you must stop and clearly explain the issue to the user.\n   5. Save the cover letter text as PDF: Use the tool save_text_as_pdf_method() which accepts string data and saves the cover letter text received as PDF and then returns the status and the location of the file\n   6. Generate email content: You need to write a professional letter to the recruiter for the job description and mention that the cover letter is attached\n   7. Your final result should print the status of each tool and the email and then mention where the cover letter is saved\n    \"\"\",\n    tools=[\n        get_job_data_method,\n        get_cover_letter_method,\n        save_text_as_pdf_method,  # Using another agent as a tool!\n    ],\n)\n\nprint(\"âœ… CoMail agent created\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:42:00.685929Z","iopub.execute_input":"2025-12-01T04:42:00.686309Z","iopub.status.idle":"2025-12-01T04:42:00.693777Z","shell.execute_reply.started":"2025-12-01T04:42:00.686283Z","shell.execute_reply":"2025-12-01T04:42:00.692764Z"}},"outputs":[{"name":"stdout","text":"âœ… CoMail agent created\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Test the currency agent\ncomail_runner = InMemoryRunner(agent=comail_agent)\n_ = await comail_runner.run_debug(\n    \"Generate the email for the latest jobs\"\n)\nprint('agent run complete')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:44:31.478127Z","iopub.execute_input":"2025-12-01T04:44:31.479180Z","iopub.status.idle":"2025-12-01T04:44:35.959593Z","shell.execute_reply.started":"2025-12-01T04:44:31.479146Z","shell.execute_reply":"2025-12-01T04:44:35.958403Z"}},"outputs":[{"name":"stdout","text":"\n ### Created new session: debug_session_id\n\nUser > Generate the email for the latest jobs\n","output_type":"stream"},{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"agent run complete\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"!pwd\n!ls -lah /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:43:12.643409Z","iopub.execute_input":"2025-12-01T04:43:12.643739Z","iopub.status.idle":"2025-12-01T04:43:12.913543Z","shell.execute_reply.started":"2025-12-01T04:43:12.643709Z","shell.execute_reply":"2025-12-01T04:43:12.912276Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\ntotal 12K\ndrwxr-xr-x 3 root root 4.0K Dec  1 03:41 .\ndrwxr-xr-x 5 root root 4.0K Dec  1 03:40 ..\ndrwxr-xr-x 2 root root 4.0K Dec  1 03:41 .virtual_documents\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}